{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2de4fc42c70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "         [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "         [0.9408, 0.1332, 0.9346, 0.5936]],\n",
      "\n",
      "        [[0.8694, 0.5677, 0.7411, 0.4294],\n",
      "         [0.8854, 0.5739, 0.2666, 0.6274],\n",
      "         [0.2696, 0.4414, 0.2969, 0.8317]]])\n",
      "tensor([[[0.1053, 0.2695, 0.3588, 0.1994],\n",
      "         [0.5472, 0.0062, 0.9516, 0.0753],\n",
      "         [0.8860, 0.5832, 0.3376, 0.8090]],\n",
      "\n",
      "        [[0.5779, 0.9040, 0.5547, 0.3423],\n",
      "         [0.6343, 0.3644, 0.7104, 0.9464],\n",
      "         [0.7890, 0.2814, 0.7886, 0.5895]]])\n",
      "tensor([[[0.7539, 0.1952, 0.0050, 0.3068],\n",
      "         [0.1165, 0.9103, 0.6440, 0.7071],\n",
      "         [0.6581, 0.4913, 0.8913, 0.1447]],\n",
      "\n",
      "        [[0.5315, 0.1587, 0.6542, 0.3278],\n",
      "         [0.6532, 0.3958, 0.9147, 0.2036],\n",
      "         [0.2018, 0.2018, 0.9497, 0.6666]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "num_heads = 2\n",
    "d_model = 4\n",
    "d_k = d_model // num_heads\n",
    "\n",
    "# Generate query, key and value matrices in the format of (batch_size, seq_len, d_model)\n",
    "query = torch.rand(batch_size, seq_len, d_model)\n",
    "key = torch.rand(batch_size, seq_len, d_model)\n",
    "value = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Current query, key and value matrixes\n",
    "print(query)\n",
    "print(key)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.8823, 0.9150],\n",
      "          [0.3829, 0.9593]],\n",
      "\n",
      "         [[0.3904, 0.6009],\n",
      "          [0.2566, 0.7936]],\n",
      "\n",
      "         [[0.9408, 0.1332],\n",
      "          [0.9346, 0.5936]]],\n",
      "\n",
      "\n",
      "        [[[0.8694, 0.5677],\n",
      "          [0.7411, 0.4294]],\n",
      "\n",
      "         [[0.8854, 0.5739],\n",
      "          [0.2666, 0.6274]],\n",
      "\n",
      "         [[0.2696, 0.4414],\n",
      "          [0.2969, 0.8317]]]])\n",
      "tensor([[[[0.1053, 0.2695],\n",
      "          [0.3588, 0.1994]],\n",
      "\n",
      "         [[0.5472, 0.0062],\n",
      "          [0.9516, 0.0753]],\n",
      "\n",
      "         [[0.8860, 0.5832],\n",
      "          [0.3376, 0.8090]]],\n",
      "\n",
      "\n",
      "        [[[0.5779, 0.9040],\n",
      "          [0.5547, 0.3423]],\n",
      "\n",
      "         [[0.6343, 0.3644],\n",
      "          [0.7104, 0.9464]],\n",
      "\n",
      "         [[0.7890, 0.2814],\n",
      "          [0.7886, 0.5895]]]])\n",
      "tensor([[[[0.7539, 0.1952],\n",
      "          [0.0050, 0.3068]],\n",
      "\n",
      "         [[0.1165, 0.9103],\n",
      "          [0.6440, 0.7071]],\n",
      "\n",
      "         [[0.6581, 0.4913],\n",
      "          [0.8913, 0.1447]]],\n",
      "\n",
      "\n",
      "        [[[0.5315, 0.1587],\n",
      "          [0.6542, 0.3278]],\n",
      "\n",
      "         [[0.6532, 0.3958],\n",
      "          [0.9147, 0.2036]],\n",
      "\n",
      "         [[0.2018, 0.2018],\n",
      "          [0.9497, 0.6666]]]])\n"
     ]
    }
   ],
   "source": [
    "# Add num heads and d_k into the matrixes: (batch_size, seq_len, d_model) --> (batch_size, seq_len, h, d_k)\n",
    "query = query.view(query.shape[0], query.shape[1], num_heads, d_k)\n",
    "key = key.view(key.shape[0], key.shape[1], num_heads, d_k)\n",
    "value = value.view(value.shape[0], value.shape[1], num_heads, d_k)\n",
    "\n",
    "print(query)\n",
    "print(key)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the query, key and value matrixes from: (batch_size, seq_len, h, d_k) --> (batch_size, h, seq_len, d_k)\n",
    "# This is because attention mechanisms processes each head individually\n",
    "query = query.transpose(1, 2)\n",
    "key = key.transpose(1, 2)\n",
    "value = value.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5349, 0.5326],\n",
       "          [0.5264, 0.5284],\n",
       "          [0.5064, 0.5538]],\n",
       "\n",
       "         [[0.5694, 0.3594],\n",
       "          [0.5591, 0.3616],\n",
       "          [0.5559, 0.4035]]],\n",
       "\n",
       "\n",
       "        [[[0.4613, 0.2454],\n",
       "          [0.4612, 0.2453],\n",
       "          [0.4656, 0.2470]],\n",
       "\n",
       "         [[0.8528, 0.3991],\n",
       "          [0.8522, 0.3927],\n",
       "          [0.8556, 0.3898]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@staticmethod\n",
    "def attention(query, key, value, mask = None):\n",
    "    d_k = query.shape[-1]\n",
    "    attention_scores = query @ key.transpose(-2, -1) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attention_scores.masked_fill(mask == 0, -1e9) # since the attention scores will be going through a softmax function, mask will be initialized as a value close to 0 such that when exp function is applied, the value will be 0\n",
    "    attention_scores = attention_scores.softmax(dim = -1)\n",
    "    return (attention_scores @ value)\n",
    "\n",
    "x = attention(query, key, value, mask = None)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5349, 0.5326, 0.5694, 0.3594],\n",
       "         [0.5264, 0.5284, 0.5591, 0.3616],\n",
       "         [0.5064, 0.5538, 0.5559, 0.4035]],\n",
       "\n",
       "        [[0.4613, 0.2454, 0.8528, 0.3991],\n",
       "         [0.4612, 0.2453, 0.8522, 0.3927],\n",
       "         [0.4656, 0.2470, 0.8556, 0.3898]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, d_k * num_heads)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer_architecture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
